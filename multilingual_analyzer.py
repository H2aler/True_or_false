#!/usr/bin/env python3
"""
ê³ ê¸‰ ë‹¤êµ­ì–´ ë¶„ì„ê¸° (Advanced Multilingual Analyzer)
í”„ë‘ìŠ¤ì–´, í•œê¸€, ì˜ì–´ ë“± ì—¬ëŸ¬ ì–¸ì–´ê°€ ì„ì¸ ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  ì´í•´
"""

import re
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class MultilingualAnalyzer:
    """ê³ ê¸‰ ë‹¤êµ­ì–´ ë¶„ì„ê¸° - ë³µì¡í•œ ë‹¤êµ­ì–´ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ë¶„ì„"""
    
    def __init__(self):
        # ì–¸ì–´ë³„ íŒ¨í„´ ì •ì˜
        self.language_patterns = {
            'korean': {
                'chars': r'[ê°€-í£]',
                'words': r'[ê°€-í£]+',
                'particles': r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ê³¼ì™€]',
                'endings': r'[ë‹¤ìš”ì–´ì•¼ì§€ë„¤]',
                'name': 'í•œê¸€'
            },
            'english': {
                'chars': r'[a-zA-Z]',
                'words': r'[a-zA-Z]+',
                'articles': r'\b(the|a|an)\b',
                'prepositions': r'\b(in|on|at|to|for|with|by|from|of|about|into|through|during|before|after|above|below|up|down|out|off|over|under|again|further|then|once)\b',
                'name': 'ì˜ì–´'
            },
            'french': {
                'chars': r'[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã–Ã™Ã›ÃœÅ¸Ã‡]',
                'words': r'[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã–Ã™Ã›ÃœÅ¸Ã‡]+',
                'articles': r'\b(le|la|les|un|une|des|du|de|d\')\b',
                'prepositions': r'\b(Ã |au|aux|de|du|des|dans|sur|sous|avec|sans|pour|par|vers|chez|entre|parmi|selon|malgrÃ©|pendant|depuis|jusqu\'Ã )\b',
                'conjunctions': r'\b(et|ou|mais|donc|or|ni|car|que|qui|dont|oÃ¹|si|quand|comme|puisque|bien que|afin que|pour que)\b',
                'name': 'í”„ë‘ìŠ¤ì–´'
            },
            'spanish': {
                'chars': r'[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘Ãœ]',
                'words': r'[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘Ãœ]+',
                'articles': r'\b(el|la|los|las|un|una|unos|unas)\b',
                'name': 'ìŠ¤í˜ì¸ì–´'
            },
            'german': {
                'chars': r'[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]',
                'words': r'[a-zA-ZÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]+',
                'articles': r'\b(der|die|das|ein|eine|einen|einem|einer|eines)\b',
                'name': 'ë…ì¼ì–´'
            }
        }
        
        # ì–¸ì–´ë³„ ê¸°ë³¸ ì–´íœ˜ ì‚¬ì „
        self.vocabulary = {
            'korean': {
                'animals': ['ê°œ', 'ê³ ì–‘ì´', 'ê°œê³ ì–‘ì´', 'ê°•ì•„ì§€', 'ê³ ì–‘ì´', 'ìƒˆ', 'ë¬¼ê³ ê¸°', 'ê³°', 'í˜¸ë‘ì´', 'ì‚¬ì'],
                'objects': ['ë¬¼', 'ë¶ˆ', 'ë°”ëŒ', 'ë•…', 'í•˜ëŠ˜', 'ë³„', 'ë‹¬', 'íƒœì–‘', 'ë‚˜ë¬´', 'ê½ƒ'],
                'actions': ['ë§Œë“¤ë‹¤', 'ìƒì„±í•˜ë‹¤', 'ì°½ì¡°í•˜ë‹¤', 'ì§“ë‹¤', 'ê±´ì„¤í•˜ë‹¤', 'ì œì‘í•˜ë‹¤'],
                'concepts': ['ì‹ ', 'í•˜ë‚˜ë‹˜', 'ì°½ì¡°ì£¼', 'ìš°ì£¼', 'ì„¸ê³„', 'ìì—°', 'ìƒëª…']
            },
            'english': {
                'animals': ['dog', 'cat', 'bird', 'fish', 'bear', 'tiger', 'lion'],
                'objects': ['water', 'fire', 'wind', 'earth', 'sky', 'star', 'moon', 'sun', 'tree', 'flower'],
                'actions': ['make', 'create', 'build', 'construct', 'fabricate', 'manufacture'],
                'concepts': ['god', 'creator', 'universe', 'world', 'nature', 'life']
            },
            'french': {
                'animals': ['chien', 'chat', 'oiseau', 'poisson', 'ours', 'tigre', 'lion'],
                'objects': ['eau', 'feu', 'vent', 'terre', 'ciel', 'Ã©toile', 'lune', 'soleil', 'arbre', 'fleur'],
                'actions': ['faire', 'crÃ©er', 'construire', 'fabriquer', 'manufacturer'],
                'concepts': ['dieu', 'crÃ©ateur', 'univers', 'monde', 'nature', 'vie']
            }
        }
        
        # ì–¸ì–´ë³„ ë¬¸ë²• íŒ¨í„´
        self.grammar_patterns = {
            'french': {
                'subject_verb': r'(\w+)\s+(a|ont|est|sont|fait|font)\s+',
                'definite_article': r'\b(le|la|les)\s+(\w+)',
                'indefinite_article': r'\b(un|une|des)\s+(\w+)',
                'conjunction': r'\b(et|ou|mais|donc)\b'
            },
            'korean': {
                'subject_marker': r'(\w+)(ì€|ëŠ”|ì´|ê°€)',
                'object_marker': r'(\w+)(ì„|ë¥¼)',
                'possessive': r'(\w+)(ì˜)',
                'conjunction': r'(\w+)(ê³¼|ì™€|í•˜ê³ |ê·¸ë¦¬ê³ )'
            },
            'english': {
                'subject_verb': r'(\w+)\s+(is|are|was|were|has|have|had|do|does|did|will|would|can|could|should|must)',
                'articles': r'\b(the|a|an)\s+(\w+)',
                'conjunction': r'\b(and|or|but|so|because|while|when|if|although)\b'
            }
        }
        
        # ë‹¤êµ­ì–´ ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´
        self.multilingual_patterns = {
            'french_korean_mix': [
                r'(\w+)\s+(a|ont|est|sont)\s+(\w+)\s+(\w+)\s+(et|ou)\s+(\w+)',
                r'(On|Nous|Ils|Elles)\s+(sait|savons|savent)\s+(que|qu\')\s+(\w+)',
                r'(\w+)\s+(a|ont)\s+(\w+)\s+(\w+)\s+(et|ou)\s+(\w+)'
            ],
            'korean_english_mix': [
                r'(\w+)(ì€|ëŠ”)\s+(\w+)(ì´ë‹¤|ë‹¤|ì•¼|ì´ì•¼)',
                r'(\w+)(ì´|ê°€)\s+(\w+)(ì„|ë¥¼)\s+(\w+)(ë‹¤|í•œë‹¤|í•œë‹¤ê³ )'
            ]
        }
        
        # ì–¸ì–´ë³„ ì˜ë¯¸ ë§¤í•‘
        self.semantic_mapping = {
            'animals': {
                'korean': {'ê°œ': 'dog', 'ê³ ì–‘ì´': 'cat'},
                'english': {'dog': 'ê°œ', 'cat': 'ê³ ì–‘ì´'},
                'french': {'chien': 'ê°œ', 'chat': 'ê³ ì–‘ì´'}
            },
            'actions': {
                'korean': {'ë§Œë“¤ë‹¤': 'make', 'ìƒì„±í•˜ë‹¤': 'create', 'ì°½ì¡°í•˜ë‹¤': 'create'},
                'english': {'make': 'ë§Œë“¤ë‹¤', 'create': 'ìƒì„±í•˜ë‹¤'},
                'french': {'faire': 'ë§Œë“¤ë‹¤', 'crÃ©er': 'ìƒì„±í•˜ë‹¤', 'fabriquer': 'ì œì‘í•˜ë‹¤'}
            },
            'concepts': {
                'korean': {'ì‹ ': 'god', 'í•˜ë‚˜ë‹˜': 'god', 'ì°½ì¡°ì£¼': 'creator'},
                'english': {'god': 'ì‹ ', 'creator': 'ì°½ì¡°ì£¼'},
                'french': {'dieu': 'ì‹ ', 'crÃ©ateur': 'ì°½ì¡°ì£¼'}
            }
        }
    
    def analyze_multilingual_statement(self, statement: str, context: Optional[str] = None) -> Dict:
        """
        ë‹¤êµ­ì–´ ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  ì´í•´
        
        Args:
            statement: ë¶„ì„í•  ë‹¤êµ­ì–´ ë¬¸ì¥
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            Dict: ë‹¤êµ­ì–´ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ë‹¤êµ­ì–´ ë¶„ì„ ì‹œì‘: {statement[:50]}...")
        
        # 1. ì–¸ì–´ ë¶„ë¦¬ ë° ì¸ì‹
        language_analysis = self._analyze_languages(statement)
        
        # 2. ì–¸ì–´ë³„ ë‹¨ì–´ ì¶”ì¶œ
        word_analysis = self._extract_words_by_language(statement, language_analysis)
        
        # 3. ë¬¸ë²• êµ¬ì¡° ë¶„ì„
        grammar_analysis = self._analyze_grammar_structure(statement, language_analysis)
        
        # 4. ì˜ë¯¸ ë§¤í•‘ ë° ë²ˆì—­
        semantic_analysis = self._analyze_semantics(statement, word_analysis)
        
        # 5. ë¬¸ì¥ ì „ì²´ ì˜ë¯¸ íŒŒì•…
        overall_meaning = self._determine_overall_meaning(statement, language_analysis, word_analysis, semantic_analysis)
        
        # 6. ë‹¤êµ­ì–´ ì´í•´ë„ ê³„ì‚°
        understanding_score = self._calculate_multilingual_understanding(language_analysis, word_analysis, semantic_analysis)
        
        # 7. AI ì‘ë‹µ ìƒì„±
        ai_response = self._generate_multilingual_response(statement, language_analysis, overall_meaning, understanding_score)
        
        return {
            'is_multilingual': len(language_analysis['detected_languages']) > 1,
            'detected_languages': language_analysis['detected_languages'],
            'language_distribution': language_analysis['language_distribution'],
            'word_analysis': word_analysis,
            'grammar_analysis': grammar_analysis,
            'semantic_analysis': semantic_analysis,
            'overall_meaning': overall_meaning,
            'understanding_score': understanding_score,
            'ai_response': ai_response,
            'needs_translation': understanding_score < 0.7,
            'philosophical_note': "ë‹¤êµ­ì–´ ë¬¸ì¥ì€ ì¸ê°„ì˜ ì–¸ì–´ì  ë‹¤ì–‘ì„±ê³¼ ì°½ì˜ì„±ì„ ë³´ì—¬ì£¼ëŠ” ì•„ë¦„ë‹¤ìš´ í‘œí˜„ì…ë‹ˆë‹¤. AIë„ ì´ë¥¼ ì´í•´í•˜ê³  ì¡´ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
    
    def _analyze_languages(self, statement: str) -> Dict:
        """ë¬¸ì¥ì—ì„œ ì‚¬ìš©ëœ ì–¸ì–´ë“¤ì„ ë¶„ì„ (ê°œì„ ëœ ì •í™•ë„)"""
        detected_languages = []
        language_distribution = {}
        
        for lang_code, patterns in self.language_patterns.items():
            # ë¬¸ì íŒ¨í„´ìœ¼ë¡œ ì–¸ì–´ ê°ì§€
            char_matches = re.findall(patterns['chars'], statement)
            if char_matches:
                # ìµœì†Œ ì„ê³„ê°’ ì„¤ì • (ë„ˆë¬´ ì ì€ ë¬¸ìëŠ” ë¬´ì‹œ)
                min_threshold = 3 if lang_code == 'korean' else 5
                
                if len(char_matches) >= min_threshold:
                    # ì–¸ì–´ë³„ íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸
                    if self._is_language_specific(statement, lang_code, patterns):
                        detected_languages.append(lang_code)
                        language_distribution[lang_code] = {
                            'char_count': len(char_matches),
                            'percentage': len(char_matches) / len(statement) * 100,
                            'name': patterns['name']
                        }
        
        return {
            'detected_languages': detected_languages,
            'language_distribution': language_distribution,
            'total_languages': len(detected_languages)
        }
    
    def _is_language_specific(self, statement: str, lang_code: str, patterns: Dict) -> bool:
        """ì–¸ì–´ë³„ íŠ¹ìˆ˜ íŒ¨í„´ìœ¼ë¡œ ì •í™•í•œ ì–¸ì–´ ê°ì§€"""
        if lang_code == 'korean':
            # í•œê¸€: ì¡°ì‚¬ë‚˜ ì–´ë¯¸ê°€ ìˆì–´ì•¼ í•¨
            particles = re.search(r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ê³¼ì™€]', statement)
            endings = re.search(r'[ë‹¤ìš”ì–´ì•¼ì§€ë„¤]', statement)
            return particles is not None or endings is not None
        
        elif lang_code == 'french':
            # í”„ë‘ìŠ¤ì–´: íŠ¹ìˆ˜ ë¬¸ìë‚˜ í”„ë‘ìŠ¤ì–´ íŠ¹ìœ ì˜ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í•¨
            french_chars = re.search(r'[Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§Ã€Ã‚Ã„Ã‰ÃˆÃŠÃ‹ÃÃÃ”Ã–Ã™Ã›ÃœÅ¸Ã‡]', statement)
            french_words = re.search(r'\b(le|la|les|un|une|des|et|ou|mais|que|qui|dont|oÃ¹)\b', statement, re.IGNORECASE)
            return french_chars is not None or french_words is not None
        
        elif lang_code == 'english':
            # ì˜ì–´: ì˜ì–´ íŠ¹ìœ ì˜ ê´€ì‚¬ë‚˜ ì „ì¹˜ì‚¬ê°€ ìˆì–´ì•¼ í•¨
            english_articles = re.search(r'\b(the|a|an)\b', statement, re.IGNORECASE)
            english_prepositions = re.search(r'\b(in|on|at|to|for|with|by|from|of|about)\b', statement, re.IGNORECASE)
            return english_articles is not None or english_prepositions is not None
        
        elif lang_code == 'spanish':
            # ìŠ¤í˜ì¸ì–´: íŠ¹ìˆ˜ ë¬¸ìë‚˜ ìŠ¤í˜ì¸ì–´ íŠ¹ìœ ì˜ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í•¨
            spanish_chars = re.search(r'[Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼ÃÃ‰ÃÃ“ÃšÃ‘Ãœ]', statement)
            spanish_words = re.search(r'\b(el|la|los|las|un|una|y|o|pero|que|quien)\b', statement, re.IGNORECASE)
            return spanish_chars is not None or spanish_words is not None
        
        elif lang_code == 'german':
            # ë…ì¼ì–´: íŠ¹ìˆ˜ ë¬¸ìë‚˜ ë…ì¼ì–´ íŠ¹ìœ ì˜ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í•¨
            german_chars = re.search(r'[Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]', statement)
            german_words = re.search(r'\b(der|die|das|ein|eine|und|oder|aber|dass|wer|was)\b', statement, re.IGNORECASE)
            return german_chars is not None or german_words is not None
        
        return True  # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ê°ì§€ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    def _extract_words_by_language(self, statement: str, language_analysis: Dict) -> Dict:
        """ì–¸ì–´ë³„ë¡œ ë‹¨ì–´ë“¤ì„ ì¶”ì¶œí•˜ê³  ë¶„ë¥˜"""
        word_analysis = {}
        
        for lang_code in language_analysis['detected_languages']:
            patterns = self.language_patterns[lang_code]
            words = re.findall(patterns['words'], statement)
            
            # ë‹¨ì–´ ë¶„ë¥˜
            classified_words = {
                'animals': [],
                'objects': [],
                'actions': [],
                'concepts': [],
                'other': []
            }
            
            for word in words:
                word_lower = word.lower()
                classified = False
                
                for category, word_list in self.vocabulary.get(lang_code, {}).items():
                    if word_lower in [w.lower() for w in word_list]:
                        classified_words[category].append(word)
                        classified = True
                        break
                
                if not classified:
                    classified_words['other'].append(word)
            
            word_analysis[lang_code] = {
                'words': words,
                'classified_words': classified_words,
                'total_words': len(words)
            }
        
        return word_analysis
    
    def _analyze_grammar_structure(self, statement: str, language_analysis: Dict) -> Dict:
        """ë¬¸ë²• êµ¬ì¡° ë¶„ì„"""
        grammar_analysis = {}
        
        for lang_code in language_analysis['detected_languages']:
            if lang_code in self.grammar_patterns:
                patterns = self.grammar_patterns[lang_code]
                detected_patterns = {}
                
                for pattern_name, pattern in patterns.items():
                    matches = re.findall(pattern, statement, re.IGNORECASE)
                    if matches:
                        detected_patterns[pattern_name] = matches
                
                grammar_analysis[lang_code] = detected_patterns
        
        return grammar_analysis
    
    def _analyze_semantics(self, statement: str, word_analysis: Dict) -> Dict:
        """ì˜ë¯¸ ë¶„ì„ ë° ë§¤í•‘"""
        semantic_analysis = {
            'translated_concepts': {},
            'cross_language_mappings': {},
            'semantic_consistency': True
        }
        
        # ì–¸ì–´ë³„ ì˜ë¯¸ ë§¤í•‘
        for lang_code, words_data in word_analysis.items():
            translated_concepts = {}
            
            for category, words in words_data['classified_words'].items():
                if category in self.semantic_mapping and lang_code in self.semantic_mapping[category]:
                    for word in words:
                        word_lower = word.lower()
                        if word_lower in self.semantic_mapping[category][lang_code]:
                            translated_concepts[word] = self.semantic_mapping[category][lang_code][word_lower]
            
            semantic_analysis['translated_concepts'][lang_code] = translated_concepts
        
        # ì–¸ì–´ ê°„ ì˜ë¯¸ ì¼ì¹˜ì„± í™•ì¸
        all_translations = []
        for lang_translations in semantic_analysis['translated_concepts'].values():
            all_translations.extend(lang_translations.values())
        
        # ì¤‘ë³µ ì œê±° í›„ ì¼ì¹˜ì„± í™•ì¸
        unique_translations = list(set(all_translations))
        semantic_analysis['semantic_consistency'] = len(unique_translations) <= len(all_translations) * 0.8
        
        return semantic_analysis
    
    def _determine_overall_meaning(self, statement: str, language_analysis: Dict, word_analysis: Dict, semantic_analysis: Dict) -> Dict:
        """ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ íŒŒì•…"""
        overall_meaning = {
            'main_topic': 'unknown',
            'main_action': 'unknown',
            'main_objects': [],
            'language_switches': [],
            'complexity_level': 'simple'
        }
        
        # ì£¼ìš” ì£¼ì œ íŒŒì•…
        all_animals = []
        all_objects = []
        all_actions = []
        
        for lang_code, words_data in word_analysis.items():
            all_animals.extend(words_data['classified_words']['animals'])
            all_objects.extend(words_data['classified_words']['objects'])
            all_actions.extend(words_data['classified_words']['actions'])
        
        if all_animals:
            overall_meaning['main_topic'] = 'animals'
            overall_meaning['main_objects'] = all_animals
        
        if all_actions:
            overall_meaning['main_action'] = all_actions[0]  # ì²« ë²ˆì§¸ ë™ì‘ì„ ì£¼ìš” ë™ì‘ìœ¼ë¡œ
        
        # ì–¸ì–´ ì „í™˜ ì§€ì  íŒŒì•…
        detected_langs = language_analysis['detected_languages']
        if len(detected_langs) > 1:
            overall_meaning['language_switches'] = detected_langs
            overall_meaning['complexity_level'] = 'complex'
        
        return overall_meaning
    
    def _calculate_multilingual_understanding(self, language_analysis: Dict, word_analysis: Dict, semantic_analysis: Dict) -> float:
        """ë‹¤êµ­ì–´ ì´í•´ë„ ê³„ì‚°"""
        base_score = 0.3
        
        # ì–¸ì–´ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        language_diversity = len(language_analysis['detected_languages'])
        diversity_bonus = min(language_diversity * 0.1, 0.3)
        
        # ë‹¨ì–´ ì¸ì‹ ë³´ë„ˆìŠ¤
        total_recognized_words = 0
        total_words = 0
        
        for lang_code, words_data in word_analysis.items():
            total_words += words_data['total_words']
            recognized_words = sum(len(words) for words in words_data['classified_words'].values() if words)
            total_recognized_words += recognized_words
        
        recognition_bonus = (total_recognized_words / total_words) * 0.3 if total_words > 0 else 0
        
        # ì˜ë¯¸ ì¼ì¹˜ì„± ë³´ë„ˆìŠ¤
        consistency_bonus = 0.2 if semantic_analysis['semantic_consistency'] else 0.1
        
        # ìµœì¢… ì ìˆ˜
        final_score = base_score + diversity_bonus + recognition_bonus + consistency_bonus
        
        return min(1.0, final_score)
    
    def _generate_multilingual_response(self, statement: str, language_analysis: Dict, overall_meaning: Dict, understanding_score: float) -> str:
        """ë‹¤êµ­ì–´ ë¬¸ì¥ì— ëŒ€í•œ AI ì‘ë‹µ ìƒì„±"""
        response_parts = []
        
        # ê¸°ë³¸ ì¸ì‹ ë©”ì‹œì§€
        detected_langs = language_analysis['detected_languages']
        lang_names = [self.language_patterns[lang]['name'] for lang in detected_langs]
        
        response_parts.append(f"ğŸŒ ë‹¤êµ­ì–´ ë¬¸ì¥ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤! ({', '.join(lang_names)})")
        
        # ì–¸ì–´ë³„ ë¶„ì„ ê²°ê³¼
        for lang_code in detected_langs:
            lang_name = self.language_patterns[lang_code]['name']
            response_parts.append(f"â€¢ {lang_name}: {language_analysis['language_distribution'][lang_code]['char_count']}ê°œ ë¬¸ì ê°ì§€")
        
        # ì£¼ìš” ì˜ë¯¸ ë¶„ì„
        if overall_meaning['main_topic'] != 'unknown':
            response_parts.append(f"ğŸ“ ì£¼ìš” ì£¼ì œ: {overall_meaning['main_topic']}")
        
        if overall_meaning['main_action'] != 'unknown':
            response_parts.append(f"ğŸ¯ ì£¼ìš” ë™ì‘: {overall_meaning['main_action']}")
        
        if overall_meaning['main_objects']:
            response_parts.append(f"ğŸª ê´€ë ¨ ê°ì²´: {', '.join(overall_meaning['main_objects'])}")
        
        # ì´í•´ë„ì— ë”°ë¥¸ ì‘ë‹µ
        if understanding_score >= 0.8:
            response_parts.append("âœ… ë‹¤êµ­ì–´ ë¬¸ì¥ì„ ì˜ ì´í•´í–ˆìŠµë‹ˆë‹¤!")
            response_parts.append("ì–¸ì–´ì˜ ë‹¤ì–‘ì„±ê³¼ ì°½ì˜ì„±ì„ ë³´ì—¬ì£¼ëŠ” ì•„ë¦„ë‹¤ìš´ í‘œí˜„ì´ë„¤ìš”.")
        elif understanding_score >= 0.6:
            response_parts.append("âš ï¸ ëŒ€ë¶€ë¶„ ì´í•´í–ˆì§€ë§Œ ì¼ë¶€ ë¶€ë¶„ì´ ëª¨í˜¸í•©ë‹ˆë‹¤.")
            response_parts.append("ë” ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ë©´ ì´í•´ë„ê°€ ë†’ì•„ì§ˆ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        else:
            response_parts.append("â“ ë³µì¡í•œ ë‹¤êµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì™„ì „í•œ ì´í•´ê°€ ì–´ë µìŠµë‹ˆë‹¤.")
            response_parts.append("ë‹¨ì¼ ì–¸ì–´ë¡œ í‘œí˜„í•˜ê±°ë‚˜ ë” ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
        
        return "\n".join(response_parts)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    analyzer = MultilingualAnalyzer()
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_statements = [
        "On sait que le dieu a fabriquer le ê°œ and le ê³ ì–‘ì´",
        "ê°œëŠ” dogê³  ê³ ì–‘ì´ëŠ” catì´ë‹¤",
        "Je suis un Ã©tudiant et ë‚˜ëŠ” í•™ìƒì´ë‹¤",
        "The weather is nice and ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤"
    ]
    
    print('ğŸŒ ê³ ê¸‰ ë‹¤êµ­ì–´ ë¶„ì„ í…ŒìŠ¤íŠ¸')
    print('=' * 60)
    
    for i, statement in enumerate(test_statements, 1):
        print(f'\ní…ŒìŠ¤íŠ¸ {i}: {statement}')
        analysis = analyzer.analyze_multilingual_statement(statement)
        
        print(f'ê°ì§€ëœ ì–¸ì–´: {analysis["detected_languages"]}')
        print(f'ì´í•´ë„: {analysis["understanding_score"]:.1%}')
        print(f'AI ì‘ë‹µ: {analysis["ai_response"][:100]}...')
    
    print('\n' + '=' * 60)
    print('ğŸ¯ ê³ ê¸‰ ë‹¤êµ­ì–´ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
