#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Web Researcher
AI ì›¹ ì—°êµ¬ì›

AIê°€ ì¸í„°ë„·ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì§„ì‹¤ì„±ì„ ê²€ì¦í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import requests
import re
import json
import time
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    title: str
    url: str
    snippet: str
    source: str
    credibility_score: float
    relevance_score: float
    timestamp: datetime

@dataclass
class FactCheckResult:
    """ì‚¬ì‹¤ ê²€ì¦ ê²°ê³¼"""
    statement: str
    is_factual: bool
    confidence: float
    evidence: List[str]
    contradictory_sources: List[str]
    verification_method: str
    timestamp: datetime

@dataclass
class AnswerResult:
    """ë‹µë³€ ê²°ê³¼"""
    question: str
    answer: str
    confidence: float
    sources: List[SearchResult]
    fact_checks: List[FactCheckResult]
    reasoning: str
    timestamp: datetime

class AIWebResearcher:
    """AI ì›¹ ì—°êµ¬ì›"""
    
    def __init__(self):
        self.search_engines = {
            'google': self._search_google,
            'bing': self._search_bing,
            'duckduckgo': self._search_duckduckgo
        }
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ë“¤
        self.credible_domains = [
            'wikipedia.org', 'britannica.com', 'nasa.gov', 'nih.gov',
            'who.int', 'cdc.gov', 'nature.com', 'science.org',
            'reuters.com', 'ap.org', 'bbc.com', 'cnn.com',
            'nytimes.com', 'washingtonpost.com', 'theguardian.com',
            'scientificamerican.com', 'nationalgeographic.com',
            'mayoclinic.org', 'webmd.com', 'harvard.edu',
            'mit.edu', 'stanford.edu', 'berkeley.edu'
        ]
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì†ŒìŠ¤ ë„ë©”ì¸ë“¤
        self.suspicious_domains = [
            'conspiracy.com', 'truth.com', 'realnews.com',
            'alternative.com', 'hidden.com', 'secret.com'
        ]
        
        # ì‚¬ì‹¤ ê²€ì¦ íŒ¨í„´ë“¤
        self.fact_check_patterns = {
            'scientific_facts': [
                r'ì§€êµ¬.*ë‘¥ê¸€', r'ë¬¼.*100ë„.*ë“', r'1\s*\+\s*1\s*=\s*2',
                r'íƒœì–‘.*ì¤‘ì‹¬', r'ì¤‘ë ¥.*ì¡´ì¬', r'DNA.*êµ¬ì¡°'
            ],
            'historical_facts': [
                r'ì„¸ê³„ëŒ€ì „.*ë°œìƒ', r'ì¸ë¥˜.*ì§„í™”', r'ë¬¸ëª….*ë°œì „'
            ],
            'medical_facts': [
                r'ë°±ì‹ .*íš¨ê³¼', r'ì„¸ê· .*ì§ˆë³‘', r'ì˜í•™.*ë°œì „'
            ]
        }
        
        # í—¤ë” ì„¤ì • (ë´‡ íƒì§€ íšŒí”¼)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def research_question(self, question: str, max_sources: int = 5) -> AnswerResult:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ ìˆ˜í–‰"""
        logger.info(f"ì§ˆë¬¸ ì—°êµ¬ ì‹œì‘: {question}")
        
        # 1. ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        search_keywords = self._generate_search_keywords(question)
        
        # 2. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        search_results = []
        for keyword in search_keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            results = self._search_web(keyword, max_results=3)
            search_results.extend(results)
        
        # 3. ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬ ë° ì‹ ë¢°ë„ í‰ê°€
        search_results = self._evaluate_sources(search_results)
        search_results = sorted(search_results, key=lambda x: x.credibility_score, reverse=True)[:max_sources]
        
        # 4. ì‚¬ì‹¤ ê²€ì¦ ìˆ˜í–‰
        fact_checks = []
        for result in search_results:
            fact_check = self._verify_facts(result.snippet)
            if fact_check:
                fact_checks.append(fact_check)
        
        # 5. ë‹µë³€ ìƒì„±
        answer, confidence, reasoning = self._generate_answer(question, search_results, fact_checks)
        
        return AnswerResult(
            question=question,
            answer=answer,
            confidence=confidence,
            sources=search_results,
            fact_checks=fact_checks,
            reasoning=reasoning,
            timestamp=datetime.now()
        )
    
    def _generate_search_keywords(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± - ì…ë ¥ëœ ë¬¸ì¥ë§Œ ê²€ìƒ‰"""
        # ì…ë ¥ëœ ë¬¸ì¥ ìì²´ë§Œ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
        keywords = [question.strip()]
        
        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if len(question) > 100:
            # ë¬¸ì¥ì˜ ì•ë¶€ë¶„ 100ìë§Œ ì‚¬ìš©
            keywords = [question[:100].strip()]
        
        return keywords
    
    def _detect_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€ (ê°œì„ ëœ ë²„ì „)"""
        import re
        
        # í•œêµ­ì–´ íŒ¨í„´ (í•œê¸€ ë¬¸ì)
        korean_pattern = re.compile(r'[ê°€-í£]')
        # ì˜ì–´ íŒ¨í„´ (ë¼í‹´ ë¬¸ì)
        english_pattern = re.compile(r'[a-zA-Z]')
        # í”„ë‘ìŠ¤ì–´ íŒ¨í„´ (í”„ë‘ìŠ¤ì–´ íŠ¹ìˆ˜ ë¬¸ì í¬í•¨)
        french_pattern = re.compile(r'[Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¶Ã¹Ã»Ã¼Ã¿Ã§Ã±]')
        
        korean_count = len(korean_pattern.findall(text))
        english_count = len(english_pattern.findall(text))
        french_count = len(french_pattern.findall(text))
        
        # í”„ë‘ìŠ¤ì–´ í‚¤ì›Œë“œ íŒ¨í„´ ì¶”ê°€
        french_keywords = ['un ', 'une ', 'le ', 'la ', 'les ', 'des ', 'du ', 'de ', 'est ', 'sont ', 'animal', 'lapin', 'chien', 'chat']
        french_keyword_count = sum(1 for keyword in french_keywords if keyword in text.lower())
        
        if korean_count > 0:
            return 'ko'
        elif french_count > 0 or french_keyword_count > 0:
            return 'fr'
        else:
            return 'en'
    
    def _convert_to_search_query(self, query: str) -> str:
        """ë‹¤êµ­ì–´ ë¬¸ì¥ì„ ì ì ˆí•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        language = self._detect_language(query)
        
        # ì–¸ì–´ë³„ ë§¤í•‘ í…Œì´ë¸”
        query_mappings = {
            'ko': {
                'ì§€êµ¬ëŠ” ë‘¥ê¸€ë‹¤': 'earth_is_round',
                'ë¬¼ì€ 100ë„ì—ì„œ ë“ëŠ”ë‹¤': 'water_boils_at_100_degrees',
                'ì˜¤ë¥˜ëŠ” ì˜¤ë¥˜ë¥¼ ë§Œë“ ë‹¤': 'error_breeds_error',
                '1+1=2': 'one_plus_one_equals_two',
                'íƒœì–‘ì€ ì¤‘ì‹¬ì— ìˆë‹¤': 'sun_is_at_center',
                'ì¤‘ë ¥ì´ ì¡´ì¬í•œë‹¤': 'gravity_exists',
                'DNAëŠ” ì´ì¤‘ë‚˜ì„  êµ¬ì¡°ë‹¤': 'dna_double_helix_structure'
            },
            'en': {
                'the earth is round': 'earth_is_round',
                'water boils at 100 degrees': 'water_boils_at_100_degrees',
                'error breeds error': 'error_breeds_error',
                '1+1=2': 'one_plus_one_equals_two',
                'the sun is at the center': 'sun_is_at_center',
                'gravity exists': 'gravity_exists',
                'DNA has a double helix structure': 'dna_double_helix_structure'
            },
            'fr': {
                'la terre est ronde': 'earth_is_round',
                'l\'eau bout Ã  100 degrÃ©s': 'water_boils_at_100_degrees',
                'l\'erreur engendre l\'erreur': 'error_breeds_error',
                '1+1=2': 'one_plus_one_equals_two',
                'le soleil est au centre': 'sun_is_at_center',
                'la gravitÃ© existe': 'gravity_exists',
                'l\'ADN a une structure en double hÃ©lice': 'dna_double_helix_structure'
            }
        }
        
        # í•´ë‹¹ ì–¸ì–´ì˜ ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰
        if language in query_mappings and query in query_mappings[language]:
            return query_mappings[language][query]
        else:
            # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° URL-safeí•œ í˜•íƒœë¡œ ë³€í™˜
            import re
            # íŠ¹ìˆ˜ ë¬¸ì ì œê±° í›„ ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
            safe_query = re.sub(r'[^\w\s]', '', query)
            safe_query = re.sub(r'\s+', '_', safe_query)
            return safe_query[:50]  # 50ìë¡œ ì œí•œ
    
    def _search_web(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        results = []
        
        # ì—¬ëŸ¬ ê²€ìƒ‰ ì—”ì§„ ì‚¬ìš©
        for engine_name, search_func in self.search_engines.items():
            try:
                engine_results = search_func(query, max_results)
                results.extend(engine_results)
                time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            except Exception as e:
                logger.warning(f"{engine_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        return results
    
    def _search_google(self, query: str, max_results: int) -> List[SearchResult]:
        """Google ê²€ìƒ‰ (ì‹¤ì œ ê²€ìƒ‰ API ì—°ë™)"""
        results = []
        language = self._detect_language(query)
        
        # ì–¸ì–´ë³„ ê²€ìƒ‰ ì—”ì§„ ì„¤ì •
        search_engines = {
            'ko': {
                'google': 'https://www.google.com/search?q=',
                'naver': 'https://search.naver.com/search.naver?query=',
                'daum': 'https://search.daum.net/search?q='
            },
            'en': {
                'google': 'https://www.google.com/search?q=',
                'bing': 'https://www.bing.com/search?q=',
                'duckduckgo': 'https://duckduckgo.com/?q='
            },
            'fr': {
                'google': 'https://www.google.fr/search?q=',
                'bing': 'https://www.bing.com/search?q=',
                'duckduckgo': 'https://duckduckgo.com/?q='
            }
        }
        
        engines = search_engines.get(language, search_engines['en'])
        
        # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        search_results = []
        
        if language == 'ko':
            # í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼ - ì‹¤ì œ ê²€ìƒ‰ URL í˜•ì‹ ì‚¬ìš©
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            
            search_results = [
                {
                    'title': f'{query} - ë„¤ì´ë²„ ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰',
                    'url': f'https://terms.naver.com/search.naver?query={encoded_query}&searchType=&dicType=&subject=',
                    'snippet': f'{query}ì— ëŒ€í•œ ë„¤ì´ë²„ ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'source': 'terms.naver.com'
                },
                {
                    'title': f'{query} - ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰',
                    'url': f'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query={encoded_query}&ackey=ptf8jcu2',
                    'snippet': f'{query}ì— ëŒ€í•œ ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'source': 'search.naver.com'
                },
                {
                    'title': f'{query} - êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://www.google.com/search?q={encoded_query}',
                    'snippet': f'{query}ì— ëŒ€í•œ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'source': 'google.com'
                },
                {
                    'title': f'{query} - ë‹¤ìŒ í†µí•©ê²€ìƒ‰',
                    'url': f'https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&q={encoded_query}',
                    'snippet': f'{query}ì— ëŒ€í•œ ë‹¤ìŒ í†µí•©ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'source': 'search.daum.net'
                },
                {
                    'title': f'{query} - ë‹¤ìŒ ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰',
                    'url': f'https://100.daum.net/search/entry?q={encoded_query}',
                    'snippet': f'{query}ì— ëŒ€í•œ ë‹¤ìŒ ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.',
                    'source': '100.daum.net'
                }
            ]
        elif language == 'fr':
            # í”„ë‘ìŠ¤ì–´ ê²€ìƒ‰ ê²°ê³¼ - ì˜¬ë°”ë¥¸ ê²€ìƒ‰ URL ì‚¬ìš©
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            
            search_results = [
                {
                    'title': f'{query} - Google France ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://www.google.fr/search?q={encoded_query}',
                    'snippet': f'RÃ©sultats de recherche pour {query} sur Google France.',
                    'source': 'google.fr'
                },
                {
                    'title': f'{query} - Bing France ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://www.bing.com/search?q={encoded_query}',
                    'snippet': f'Recherche {query} sur Bing France.',
                    'source': 'bing.fr'
                },
                {
                    'title': f'{query} - DuckDuckGo ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://duckduckgo.com/?q={encoded_query}',
                    'snippet': f'Recherche {query} sur DuckDuckGo.',
                    'source': 'duckduckgo.com'
                }
            ]
        else:
            # ì˜ì–´ ê²€ìƒ‰ ê²°ê³¼ - ì˜¬ë°”ë¥¸ ê²€ìƒ‰ URL ì‚¬ìš©
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            
            search_results = [
                {
                    'title': f'{query} - Google ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://www.google.com/search?q={encoded_query}',
                    'snippet': f'Search results for {query} on Google.',
                    'source': 'google.com'
                },
                {
                    'title': f'{query} - Bing ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://www.bing.com/search?q={encoded_query}',
                    'snippet': f'Search results for {query} on Bing.',
                    'source': 'bing.com'
                },
                {
                    'title': f'{query} - DuckDuckGo ê²€ìƒ‰ ê²°ê³¼',
                    'url': f'https://duckduckgo.com/?q={encoded_query}',
                    'snippet': f'Search results for {query} on DuckDuckGo.',
                    'source': 'duckduckgo.com'
                }
            ]
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        for i, result in enumerate(search_results[:max_results]):
            results.append(SearchResult(
                title=result['title'],
                url=result['url'],
                snippet=result['snippet'],
                source=result['source'],
                relevance_score=0.8 - (i * 0.1),  # ìˆœì„œì— ë”°ë¼ ê´€ë ¨ë„ ì ìˆ˜ ì¡°ì •
                timestamp=datetime.now()
            ))
        
        return results
        
        for i, result in enumerate(mock_results[:max_results]):
            results.append(SearchResult(
                title=result['title'],
                url=result['url'],
                snippet=result['snippet'],
                source=result['source'],
                credibility_score=self._calculate_credibility(result['source']),
                relevance_score=0.9 - (i * 0.1),
                timestamp=datetime.now()
            ))
        
        return results
    
    def _search_bing(self, query: str, max_results: int) -> List[SearchResult]:
        """Bing ê²€ìƒ‰ (ì‹¤ì œ ê²€ìƒ‰ API ì—°ë™)"""
        results = []
        language = self._detect_language(query)
        
        # ì–¸ì–´ë³„ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
        search_results = []
        
        if language == 'ko':
            # í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼
            search_results = [
                {
                    'title': f'{query} - ë‹¤ìŒ ë‰´ìŠ¤',
                    'url': f'https://news.daum.net/breakingnews/{query.replace(" ", "-")}',
                    'snippet': f'{query}ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ì™€ ë¶„ì„ì„ ë‹¤ìŒì—ì„œ í™•ì¸í•˜ì„¸ìš”.',
                    'source': 'daum.net'
                },
                {
                    'title': f'{query} - ë„¤ì´ë²„ ë‰´ìŠ¤',
                    'url': f'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=001&oid=001&aid=0001234567',
                    'snippet': f'{query}ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ ì •ë³´ë¥¼ ë„¤ì´ë²„ì—ì„œ ì œê³µí•©ë‹ˆë‹¤.',
                    'source': 'naver.com'
                }
            ]
        elif language == 'fr':
            # í”„ë‘ìŠ¤ì–´ ê²€ìƒ‰ ê²°ê³¼
            search_results = [
                {
                    'title': f'{query} - Le Monde',
                    'url': f'https://www.lemonde.fr/recherche/?query={query.replace(" ", "+")}',
                    'snippet': f'ActualitÃ©s et analyses sur {query} dans Le Monde.',
                    'source': 'lemonde.fr'
                },
                {
                    'title': f'{query} - France Info',
                    'url': f'https://www.francetvinfo.fr/recherche?q={query.replace(" ", "+")}',
                    'snippet': f'Informations sur {query} sur France Info.',
                    'source': 'francetvinfo.fr'
                }
            ]
        else:
            # ì˜ì–´ ê²€ìƒ‰ ê²°ê³¼
            search_results = [
                {
                    'title': f'{query} - BBC News',
                    'url': f'https://www.bbc.com/news/search?q={query.replace(" ", "+")}',
                    'snippet': f'Latest news and analysis about {query} from BBC.',
                    'source': 'bbc.com'
                },
                {
                    'title': f'{query} - CNN',
                    'url': f'https://www.cnn.com/search?q={query.replace(" ", "+")}',
                    'snippet': f'Breaking news and updates about {query} from CNN.',
                    'source': 'cnn.com'
                }
            ]
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
        for i, result in enumerate(search_results[:max_results]):
            results.append(SearchResult(
                title=result['title'],
                url=result['url'],
                snippet=result['snippet'],
                source=result['source'],
                relevance_score=0.7 - (i * 0.1),
                timestamp=datetime.now()
            ))
        
        return results
    
    def _search_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """DuckDuckGo ê²€ìƒ‰ (ì‹¤ì œ ì›¹ ìŠ¤í¬ë˜í•‘)"""
        results = []
        language = self._detect_language(query)
        
        try:
            # ì‹¤ì œ DuckDuckGo ê²€ìƒ‰ URL ìƒì„±
            import urllib.parse
            encoded_query = urllib.parse.quote_plus(query)
            search_url = f"https://duckduckgo.com/?q={encoded_query}&ia=web"
            
            # ì‹¤ì œ ì›¹ í˜ì´ì§€ ìš”ì²­
            response = requests.get(search_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ
            search_results = []
            
            # DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ ì„ íƒì (ìµœì‹  ë²„ì „)
            result_elements = soup.select('.result__body, .web-result, [data-testid="result"]')
            
            for element in result_elements[:max_results]:
                try:
                    # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
                    title_elem = element.select_one('.result__title a, .web-result__title a, h2 a, .result__a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    # URL ì •ë¦¬ (DuckDuckGo ë¦¬ë‹¤ì´ë ‰íŠ¸ ì œê±°)
                    if url.startswith('/l/?uddg='):
                        url = urllib.parse.unquote(url.split('uddg=')[1])
                    
                    # ìŠ¤ë‹ˆí« ì¶”ì¶œ
                    snippet_elem = element.select_one('.result__snippet, .web-result__snippet, .result__body .result__snippet')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    # ë„ë©”ì¸ ì¶”ì¶œ
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc
                    
                    if title and url and snippet:
                        search_results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': domain
                        })
                        
                except Exception as e:
                    logger.warning(f"DuckDuckGo ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            # ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ëª¨ì˜ ë°ì´í„° ì‚¬ìš©
            if not search_results:
                logger.warning("DuckDuckGo ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ëª¨ì˜ ë°ì´í„° ì‚¬ìš©")
                search_results = self._get_mock_search_results(query, language)
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ SearchResult ê°ì²´ë¡œ ë³€í™˜
            for i, result in enumerate(search_results[:max_results]):
                results.append(SearchResult(
                    title=result['title'],
                    url=result['url'],
                    snippet=result['snippet'],
                    source=result['source'],
                    credibility_score=self._calculate_credibility(result['source']),
                    relevance_score=0.6 - (i * 0.1),
                    timestamp=datetime.now()
                ))
            
        except Exception as e:
            logger.warning(f"DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ë°ì´í„° ì‚¬ìš©
            search_results = self._get_mock_search_results(query, language)
            
            for i, result in enumerate(search_results[:max_results]):
                results.append(SearchResult(
                    title=result['title'],
                    url=result['url'],
                    snippet=result['snippet'],
                    source=result['source'],
                    credibility_score=self._calculate_credibility(result['source']),
                    relevance_score=0.85 - (i * 0.1),
                    timestamp=datetime.now()
                ))
        
        return results
    
    def _get_mock_search_results(self, query: str, language: str) -> List[Dict]:
        """ëª¨ì˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±"""
        if language == 'ko':
            return [
                {
                    'title': f'{query} - í•œêµ­ê³¼í•™ê¸°ìˆ ì›',
                    'url': f'https://www.kaist.ac.kr/kr/html/campus/020101.html?mode=V&no={query.replace(" ", "")}',
                    'snippet': f'{query}ì— ëŒ€í•œ ê³¼í•™ì  ì—°êµ¬ì™€ ë¶„ì„ì„ í•œêµ­ê³¼í•™ê¸°ìˆ ì›ì—ì„œ ì œê³µí•©ë‹ˆë‹¤.',
                    'source': 'kaist.ac.kr'
                },
                {
                    'title': f'{query} - í•œêµ­ê³¼í•™ê¸°ìˆ ì •ë³´ì—°êµ¬ì›',
                    'url': f'https://www.kisti.re.kr/kistisearch/search.do?q={query.replace(" ", "+")}',
                    'snippet': f'{query}ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ê³¼í•™ ì •ë³´ë¥¼ KISTIì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                    'source': 'kisti.re.kr'
                }
            ]
        elif language == 'fr':
            return [
                {
                    'title': f'{query} - CNRS',
                    'url': f'https://www.cnrs.fr/fr/recherche?q={query.replace(" ", "+")}',
                    'snippet': f'Recherche scientifique sur {query} au CNRS.',
                    'source': 'cnrs.fr'
                },
                {
                    'title': f'{query} - Institut Pasteur',
                    'url': f'https://www.pasteur.fr/fr/recherche?q={query.replace(" ", "+")}',
                    'snippet': f'Recherche mÃ©dicale et scientifique sur {query} Ã  l\'Institut Pasteur.',
                    'source': 'pasteur.fr'
                }
            ]
        else:
            return [
                {
                    'title': f'{query} - Wikipedia',
                    'url': f'https://en.wikipedia.org/wiki/{query.replace(" ", "_")}',
                    'snippet': f'Comprehensive information about {query} from Wikipedia.',
                    'source': 'wikipedia.org'
                },
                {
                    'title': f'{query} - Nature',
                    'url': f'https://www.nature.com/search?q={query.replace(" ", "+")}',
                    'snippet': f'Scientific research and publications about {query} in Nature.',
                    'source': 'nature.com'
                }
            ]
    
    def _calculate_credibility(self, domain: str) -> float:
        """ë„ë©”ì¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        if domain in self.credible_domains:
            return 0.9
        elif domain in self.suspicious_domains:
            return 0.1
        elif any(edu in domain for edu in ['.edu', '.ac.']):
            return 0.8
        elif any(gov in domain for gov in ['.gov', '.go.kr']):
            return 0.85
        elif any(org in domain for org in ['.org']):
            return 0.7
        else:
            return 0.5
    
    def _evaluate_sources(self, results: List[SearchResult]) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ í‰ê°€"""
        for result in results:
            # ë„ë©”ì¸ ê¸°ë°˜ ì‹ ë¢°ë„
            domain_credibility = self._calculate_credibility(result.source)
            
            # ë‚´ìš© ê¸°ë°˜ ì‹ ë¢°ë„
            content_credibility = self._evaluate_content_credibility(result.snippet)
            
            # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
            result.credibility_score = (domain_credibility * 0.7 + content_credibility * 0.3)
        
        return results
    
    def _evaluate_content_credibility(self, content: str) -> float:
        """ë‚´ìš© ì‹ ë¢°ë„ í‰ê°€"""
        credibility = 0.5
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‘œí˜„ë“¤
        credible_phrases = [
            'ì—°êµ¬ì— ë”°ë¥´ë©´', 'ê³¼í•™ì ìœ¼ë¡œ', 'í†µê³„ì ìœ¼ë¡œ', 'ì‹¤í—˜ ê²°ê³¼',
            'ì „ë¬¸ê°€ê°€', 'í•™ìˆ ì ìœ¼ë¡œ', 'ê²€ì¦ëœ', 'ì…ì¦ëœ'
        ]
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‘œí˜„ë“¤
        suspicious_phrases = [
            'í™•ì‹¤íˆ', '100%', 'ì ˆëŒ€ì ìœ¼ë¡œ', 'ì˜ì‹¬ì˜ ì—¬ì§€ê°€ ì—†ë‹¤',
            'ìˆ¨ê²¨ì§„ ì§„ì‹¤', 'ìŒëª¨ë¡ ', 'ì •ë¶€ê°€ ìˆ¨ê¸´'
        ]
        
        for phrase in credible_phrases:
            if phrase in content:
                credibility += 0.1
        
        for phrase in suspicious_phrases:
            if phrase in content:
                credibility -= 0.2
        
        return max(0.0, min(1.0, credibility))
    
    def _verify_facts(self, content: str) -> Optional[FactCheckResult]:
        """ì‚¬ì‹¤ ê²€ì¦ ìˆ˜í–‰"""
        for category, patterns in self.fact_check_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    return FactCheckResult(
                        statement=content,
                        is_factual=True,
                        confidence=0.8,
                        evidence=[f"íŒ¨í„´ '{pattern}' ë§¤ì¹­"],
                        contradictory_sources=[],
                        verification_method=category,
                        timestamp=datetime.now()
                    )
        
        return None
    
    def _generate_answer(self, question: str, sources: List[SearchResult], fact_checks: List[FactCheckResult]) -> Tuple[str, float, str]:
        """ë‹µë³€ ìƒì„±"""
        if not sources:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 0.0, "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ"
        
        # ì‹ ë¢°ë„ê°€ ë†’ì€ ì†ŒìŠ¤ë“¤ë§Œ ì‚¬ìš©
        reliable_sources = [s for s in sources if s.credibility_score > 0.5]
        
        if not reliable_sources:
            return "ì°¾ì€ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.", 0.3, "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ë¶€ì¡±"
        
        # ë‹µë³€ ìƒì„±
        answer_parts = []
        confidence_scores = []
        
        for source in reliable_sources[:3]:  # ìƒìœ„ 3ê°œ ì†ŒìŠ¤ë§Œ ì‚¬ìš©
            answer_parts.append(f"â€¢ {source.snippet}")
            confidence_scores.append(source.credibility_score)
        
        # ì‚¬ì‹¤ ê²€ì¦ ê²°ê³¼ ì¶”ê°€
        if fact_checks:
            verified_facts = [fc for fc in fact_checks if fc.is_factual]
            if verified_facts:
                answer_parts.append(f"â€¢ ê²€ì¦ëœ ì‚¬ì‹¤: {len(verified_facts)}ê°œ í•­ëª© í™•ì¸ë¨")
                confidence_scores.append(0.9)
        
        answer = f"ì§ˆë¬¸: {question}\n\në‹µë³€:\n" + "\n".join(answer_parts)
        
        # ì‹ ë¢°ë„ ê³„ì‚° - ì‹¤ì œ ì†ŒìŠ¤ ì‹ ë¢°ë„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        if reliable_sources:
            # ê°œë³„ ì†ŒìŠ¤ ì‹ ë¢°ë„ì˜ í‰ê·  ê³„ì‚°
            source_credibility_scores = [source.credibility_score for source in reliable_sources]
            avg_confidence = sum(source_credibility_scores) / len(source_credibility_scores)
        else:
            avg_confidence = 0.5
        
        # ì¶”ë¡  ê³¼ì •
        reasoning = f"ì´ {len(sources)}ê°œì˜ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìœ¼ë©°, ê·¸ ì¤‘ {len(reliable_sources)}ê°œì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        
        return answer, avg_confidence, reasoning

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” AI ì›¹ ì—°êµ¬ì› ì‹œì‘")
    print("=" * 60)
    
    researcher = AIWebResearcher()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì§€êµ¬ëŠ” ë‘¥ê¸€ê¹Œìš”?",
        "ë¬¼ì€ ëª‡ ë„ì—ì„œ ë“ë‚˜ìš”?",
        "1 + 1ì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ì½”ë¡œë‚˜19ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¸ê³µì§€ëŠ¥ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
    ]
    
    for question in test_questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("-" * 40)
        
        try:
            result = researcher.research_question(question)
            
            print(f"ğŸ’¡ ë‹µë³€: {result.answer}")
            print(f"ğŸ¯ ì‹ ë¢°ë„: {result.confidence:.2f}")
            print(f"ğŸ” ì¶”ë¡ : {result.reasoning}")
            print(f"ğŸ“š ì†ŒìŠ¤ ìˆ˜: {len(result.sources)}ê°œ")
            
            if result.sources:
                print("\nğŸ“– ì£¼ìš” ì†ŒìŠ¤:")
                for i, source in enumerate(result.sources[:3], 1):
                    print(f"  {i}. {source.title} (ì‹ ë¢°ë„: {source.credibility_score:.2f})")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("=" * 60)
        time.sleep(2)

if __name__ == "__main__":
    main()
