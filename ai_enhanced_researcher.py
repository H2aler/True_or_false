#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Enhanced Web Researcher
AI ê³ ê¸‰ ì›¹ ì—°êµ¬ì› (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)

ì‹¤ì‹œê°„ ê²€ìƒ‰ ê³¼ì •ì„ í•œêµ­ì–´ë¡œ í‘œì‹œí•˜ê³  ìƒì„¸í•œ ì§„í–‰ ìƒí™©ì„ ë³´ì—¬ì£¼ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import requests
import re
import json
import time
import logging
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from datetime import datetime
from urllib.parse import quote, urljoin, urlparse
from bs4 import BeautifulSoup
import random
import hashlib
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchProgress:
    """ê²€ìƒ‰ ì§„í–‰ ìƒí™©"""
    step: str
    description: str
    status: str  # 'started', 'in_progress', 'completed', 'failed'
    details: str
    timestamp: datetime
    progress_percentage: int

@dataclass
class EnhancedSearchResult:
    """í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼"""
    title: str
    url: str
    content: str
    snippet: str
    domain: str
    credibility_score: float
    relevance_score: float
    fact_check_score: float
    search_engine: str
    search_keyword: str
    processing_time: float
    timestamp: datetime

@dataclass
class EnhancedResearchAnswer:
    """í–¥ìƒëœ ì—°êµ¬ ë‹µë³€"""
    question: str
    answer: str
    confidence: float
    sources: List[EnhancedSearchResult]
    fact_verifications: List[Dict]
    reasoning: str
    limitations: List[str]
    search_progress: List[SearchProgress]
    total_processing_time: float
    timestamp: datetime

class AIEnhancedResearcher:
    """AI í–¥ìƒëœ ì›¹ ì—°êµ¬ì›"""
    
    def __init__(self, progress_callback: Optional[Callable] = None):
        self.progress_callback = progress_callback
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ë“¤
        self.credible_sources = {
            'academic': [
                'nature.com', 'science.org', 'cell.com', 'nejm.org',
                'pubmed.ncbi.nlm.nih.gov', 'arxiv.org', 'scholar.google.com'
            ],
            'news': [
                'reuters.com', 'ap.org', 'bbc.com', 'npr.org',
                'nytimes.com', 'washingtonpost.com', 'theguardian.com'
            ],
            'government': [
                'nasa.gov', 'nih.gov', 'cdc.gov', 'who.int',
                'fda.gov', 'epa.gov', 'nsf.gov'
            ],
            'encyclopedia': [
                'wikipedia.org', 'britannica.com', 'encyclopedia.com'
            ],
            'medical': [
                'mayoclinic.org', 'webmd.com', 'healthline.com',
                'medlineplus.gov', 'uptodate.com'
            ]
        }
        
        # ê²€ìƒ‰ ì—”ì§„ë³„ ì„¤ì •
        self.search_engines = {
            'google': {
                'name': 'êµ¬ê¸€',
                'search_func': self._search_google,
                'weight': 0.4
            },
            'bing': {
                'name': 'ë¹™',
                'search_func': self._search_bing,
                'weight': 0.3
            },
            'duckduckgo': {
                'name': 'ë•ë•ê³ ',
                'search_func': self._search_duckduckgo,
                'weight': 0.3
            }
        }
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def research_question(self, question: str, max_sources: int = 10) -> EnhancedResearchAnswer:
        """ì§ˆë¬¸ì— ëŒ€í•œ í–¥ìƒëœ ì—°êµ¬ ìˆ˜í–‰"""
        start_time = time.time()
        search_progress = []
        
        self._update_progress(search_progress, "ì§ˆë¬¸ ë¶„ì„", "ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        
        # 1. ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
        search_strategy = self._analyze_question(question)
        self._update_progress(search_progress, "ì§ˆë¬¸ ë¶„ì„", f"ê²€ìƒ‰ í‚¤ì›Œë“œ {len(search_strategy['keywords'])}ê°œ ìƒì„± ì™„ë£Œ", "completed")
        
        # 2. ë‹¤ì¤‘ ê²€ìƒ‰ ìˆ˜í–‰
        all_results = []
        total_engines = len(self.search_engines)
        
        for i, (engine_id, engine_info) in enumerate(self.search_engines.items()):
            self._update_progress(search_progress, f"{engine_info['name']} ê²€ìƒ‰", 
                                f"{engine_info['name']}ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
            
            for j, keyword in enumerate(search_strategy['keywords'][:3]):  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                self._update_progress(search_progress, f"{engine_info['name']} ê²€ìƒ‰", 
                                    f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘... ({j+1}/3)", "in_progress")
                
                try:
                    results = engine_info['search_func'](keyword, max_results=3)
                    for result in results:
                        result.search_engine = engine_info['name']
                        result.search_keyword = keyword
                    all_results.extend(results)
                    
                    self._update_progress(search_progress, f"{engine_info['name']} ê²€ìƒ‰", 
                                        f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì™„ë£Œ - {len(results)}ê°œ ê²°ê³¼", "completed")
                    
                except Exception as e:
                    self._update_progress(search_progress, f"{engine_info['name']} ê²€ìƒ‰", 
                                        f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}", "failed")
                
                time.sleep(0.5)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            
            self._update_progress(search_progress, f"{engine_info['name']} ê²€ìƒ‰", 
                                f"{engine_info['name']} ê²€ìƒ‰ ì™„ë£Œ - ì´ {len([r for r in all_results if r.search_engine == engine_info['name']])}ê°œ ê²°ê³¼", "completed")
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        self._update_progress(search_progress, "ê²°ê³¼ ì •ë¦¬", "ì¤‘ë³µëœ ê²°ê³¼ë¥¼ ì œê±°í•˜ê³  ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        unique_results = self._deduplicate_results(all_results)
        self._update_progress(search_progress, "ê²°ê³¼ ì •ë¦¬", f"ì¤‘ë³µ ì œê±° ì™„ë£Œ - {len(unique_results)}ê°œ ê³ ìœ  ê²°ê³¼", "completed")
        
        # 4. ì‹ ë¢°ë„ ë° ê´€ë ¨ì„± í‰ê°€
        self._update_progress(search_progress, "ì‹ ë¢°ë„ í‰ê°€", "ê²€ìƒ‰ ê²°ê³¼ì˜ ì‹ ë¢°ë„ì™€ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        evaluated_results = self._evaluate_results(unique_results, question)
        self._update_progress(search_progress, "ì‹ ë¢°ë„ í‰ê°€", f"ì‹ ë¢°ë„ í‰ê°€ ì™„ë£Œ - í‰ê·  ì‹ ë¢°ë„: {sum(r.credibility_score for r in evaluated_results)/len(evaluated_results):.2f}", "completed")
        
        # 5. ìƒìœ„ ì†ŒìŠ¤ ì„ íƒ
        self._update_progress(search_progress, "ì†ŒìŠ¤ ì„ íƒ", "ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ë“¤ì„ ì„ íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        top_results = sorted(evaluated_results, key=lambda x: x.credibility_score, reverse=True)[:max_sources]
        self._update_progress(search_progress, "ì†ŒìŠ¤ ì„ íƒ", f"ìƒìœ„ {len(top_results)}ê°œ ì†ŒìŠ¤ ì„ íƒ ì™„ë£Œ", "completed")
        
        # 6. ì‚¬ì‹¤ ê²€ì¦ ìˆ˜í–‰
        self._update_progress(search_progress, "ì‚¬ì‹¤ ê²€ì¦", "ì„ íƒëœ ì†ŒìŠ¤ë“¤ì˜ ì‚¬ì‹¤ì„±ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        fact_verifications = self._verify_facts(top_results, question)
        self._update_progress(search_progress, "ì‚¬ì‹¤ ê²€ì¦", f"ì‚¬ì‹¤ ê²€ì¦ ì™„ë£Œ - {len(fact_verifications)}ê°œ í•­ëª© ê²€ì¦ë¨", "completed")
        
        # 7. ë‹µë³€ ìƒì„±
        self._update_progress(search_progress, "ë‹µë³€ ìƒì„±", "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "started")
        answer, confidence, reasoning, limitations = self._generate_comprehensive_answer(
            question, top_results, fact_verifications
        )
        self._update_progress(search_progress, "ë‹µë³€ ìƒì„±", f"ë‹µë³€ ìƒì„± ì™„ë£Œ - ì‹ ë¢°ë„: {confidence:.2f}", "completed")
        
        total_time = time.time() - start_time
        
        return EnhancedResearchAnswer(
            question=question,
            answer=answer,
            confidence=confidence,
            sources=top_results,
            fact_verifications=fact_verifications,
            reasoning=reasoning,
            limitations=limitations,
            search_progress=search_progress,
            total_processing_time=total_time,
            timestamp=datetime.now()
        )
    
    def _update_progress(self, progress_list: List[SearchProgress], step: str, description: str, status: str, details: str = ""):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        progress = SearchProgress(
            step=step,
            description=description,
            status=status,
            details=details,
            timestamp=datetime.now(),
            progress_percentage=min(100, len(progress_list) * 10)  # ê°„ë‹¨í•œ ì§„í–‰ë¥  ê³„ì‚°
        )
        progress_list.append(progress)
        
        # ì½œë°± í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
        if self.progress_callback:
            self.progress_callback(progress)
        
        # ë¡œê·¸ ì¶œë ¥
        status_emoji = {
            'started': 'ğŸš€',
            'in_progress': 'â³',
            'completed': 'âœ…',
            'failed': 'âŒ'
        }
        
        print(f"{status_emoji.get(status, 'ğŸ“')} {step}: {description}")
        if details:
            print(f"   â””â”€ {details}")
    
    def _analyze_question(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½"""
        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        question_types = []
        if any(word in question for word in ['ë¬´ì—‡', 'ë­', 'what']):
            question_types.append('definition')
        if any(word in question for word in ['ì–¸ì œ', 'when']):
            question_types.append('temporal')
        if any(word in question for word in ['ì–´ë””', 'where']):
            question_types.append('location')
        if any(word in question for word in ['ì™œ', 'why']):
            question_types.append('causal')
        if any(word in question for word in ['ì–´ë–»ê²Œ', 'how']):
            question_types.append('process')
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        keywords = [question]
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ í‚¤ì›Œë“œ ì¶”ê°€
        if 'definition' in question_types:
            keywords.append(question + ' ì •ì˜ ì˜ë¯¸')
        if 'temporal' in question_types:
            keywords.append(question + ' ë‚ ì§œ ì‹œê°„')
        if 'location' in question_types:
            keywords.append(question + ' ìœ„ì¹˜ ì¥ì†Œ')
        if 'causal' in question_types:
            keywords.append(question + ' ì´ìœ  ì›ì¸')
        if 'process' in question_types:
            keywords.append(question + ' ë°©ë²• ê³¼ì •')
        
        # ì˜ì–´ í‚¤ì›Œë“œ ì¶”ê°€
        if not any(ord(char) > 127 for char in question):
            keywords.append(question + ' facts information research')
        
        return {
            'types': question_types,
            'keywords': keywords[:5],  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            'complexity': len(question.split()) / 10.0
        }
    
    def _search_google(self, query: str, max_results: int) -> List[EnhancedSearchResult]:
        """êµ¬ê¸€ ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        results = []
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ê²€ìƒ‰ ê²°ê³¼
        mock_results = [
            {
                'title': f'{query}ì— ëŒ€í•œ ì •ë³´ - Wikipedia',
                'url': f'https://ko.wikipedia.org/wiki/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ê°€ ìœ„í‚¤í”¼ë””ì•„ì— ìˆìŠµë‹ˆë‹¤.',
                'domain': 'wikipedia.org'
            },
            {
                'title': f'{query} ê´€ë ¨ ë‰´ìŠ¤ - BBC',
                'url': f'https://www.bbc.com/news/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ì™€ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.',
                'domain': 'bbc.com'
            },
            {
                'title': f'{query} ê³¼í•™ì  ì—°êµ¬ - Nature',
                'url': f'https://www.nature.com/articles/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ê³¼í•™ì  ì—°êµ¬ ê²°ê³¼ì™€ ë…¼ë¬¸ì…ë‹ˆë‹¤.',
                'domain': 'nature.com'
            }
        ]
        
        for i, result in enumerate(mock_results[:max_results]):
            start_time = time.time()
            # ì‹¤ì œ ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì‹œë®¬ë ˆì´ì…˜
            content = self._fetch_web_content(result['url'])
            processing_time = time.time() - start_time
            
            results.append(EnhancedSearchResult(
                title=result['title'],
                url=result['url'],
                content=content,
                snippet=result['snippet'],
                domain=result['domain'],
                credibility_score=self._calculate_credibility(result['domain']),
                relevance_score=0.9 - (i * 0.1),
                fact_check_score=0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
                search_engine='êµ¬ê¸€',
                search_keyword=query,
                processing_time=processing_time,
                timestamp=datetime.now()
            ))
        
        return results
    
    def _search_bing(self, query: str, max_results: int) -> List[EnhancedSearchResult]:
        """ë¹™ ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        results = []
        
        mock_results = [
            {
                'title': f'{query} - Britannica',
                'url': f'https://www.britannica.com/topic/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ë°±ê³¼ì‚¬ì „ ì •ë³´ì…ë‹ˆë‹¤.',
                'domain': 'britannica.com'
            },
            {
                'title': f'{query} ì˜í•™ ì •ë³´ - Mayo Clinic',
                'url': f'https://www.mayoclinic.org/diseases-conditions/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ì˜í•™ì  ì •ë³´ì™€ ì¹˜ë£Œë²•ì…ë‹ˆë‹¤.',
                'domain': 'mayoclinic.org'
            }
        ]
        
        for i, result in enumerate(mock_results[:max_results]):
            start_time = time.time()
            content = self._fetch_web_content(result['url'])
            processing_time = time.time() - start_time
            
            results.append(EnhancedSearchResult(
                title=result['title'],
                url=result['url'],
                content=content,
                snippet=result['snippet'],
                domain=result['domain'],
                credibility_score=self._calculate_credibility(result['domain']),
                relevance_score=0.8 - (i * 0.1),
                fact_check_score=0.0,
                search_engine='ë¹™',
                search_keyword=query,
                processing_time=processing_time,
                timestamp=datetime.now()
            ))
        
        return results
    
    def _search_duckduckgo(self, query: str, max_results: int) -> List[EnhancedSearchResult]:
        """ë•ë•ê³  ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        results = []
        
        mock_results = [
            {
                'title': f'{query} - Scientific American',
                'url': f'https://www.scientificamerican.com/article/{quote(query)}',
                'snippet': f'{query}ì— ëŒ€í•œ ê³¼í•™ì  ì„¤ëª…ê³¼ ë¶„ì„ì…ë‹ˆë‹¤.',
                'domain': 'scientificamerican.com'
            }
        ]
        
        for i, result in enumerate(mock_results[:max_results]):
            start_time = time.time()
            content = self._fetch_web_content(result['url'])
            processing_time = time.time() - start_time
            
            results.append(EnhancedSearchResult(
                title=result['title'],
                url=result['url'],
                content=content,
                snippet=result['snippet'],
                domain=result['domain'],
                credibility_score=self._calculate_credibility(result['domain']),
                relevance_score=0.85 - (i * 0.1),
                fact_check_score=0.0,
                search_engine='ë•ë•ê³ ',
                search_keyword=query,
                processing_time=processing_time,
                timestamp=datetime.now()
            ))
        
        return results
    
    def _fetch_web_content(self, url: str) -> str:
        """ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì‹¤ì œë¡œëŠ” requestsë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜´
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style"]):
                script.decompose()
            
            text = soup.get_text()
            return text[:2000]  # ì²˜ìŒ 2000ìë§Œ ì‚¬ìš©
            
        except Exception as e:
            logger.warning(f"ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {url}: {e}")
            return f"ì›¹ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}"
    
    def _deduplicate_results(self, results: List[EnhancedSearchResult]) -> List[EnhancedSearchResult]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        return unique_results
    
    def _evaluate_results(self, results: List[EnhancedSearchResult], question: str) -> List[EnhancedSearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ í‰ê°€"""
        for result in results:
            # ë„ë©”ì¸ ì‹ ë¢°ë„
            domain_credibility = self._calculate_credibility(result.domain)
            
            # ë‚´ìš© ì‹ ë¢°ë„
            content_credibility = self._calculate_content_credibility(result.content)
            
            # ê´€ë ¨ì„± ì ìˆ˜
            relevance_score = self._calculate_relevance(result, question)
            
            # ì‚¬ì‹¤ ê²€ì¦ ì ìˆ˜
            fact_check_score = self._calculate_fact_check_score(result.content)
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            result.credibility_score = (
                domain_credibility * 0.4 +
                content_credibility * 0.3 +
                relevance_score * 0.2 +
                fact_check_score * 0.1
            )
            result.relevance_score = relevance_score
            result.fact_check_score = fact_check_score
        
        return results
    
    def _calculate_credibility(self, domain: str) -> float:
        """ë„ë©”ì¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        for category, sources in self.credible_sources.items():
            if domain in sources:
                if category == 'academic':
                    return 0.95
                elif category == 'government':
                    return 0.9
                elif category == 'news':
                    return 0.8
                elif category == 'encyclopedia':
                    return 0.75
                elif category == 'medical':
                    return 0.85
        
        # ë„ë©”ì¸ í™•ì¥ì ê¸°ë°˜ í‰ê°€
        if domain.endswith('.edu'):
            return 0.9
        elif domain.endswith('.gov'):
            return 0.85
        elif domain.endswith('.org'):
            return 0.7
        elif domain.endswith('.com'):
            return 0.6
        else:
            return 0.5
    
    def _calculate_content_credibility(self, content: str) -> float:
        """ë‚´ìš© ì‹ ë¢°ë„ ê³„ì‚°"""
        credibility = 0.5
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‘œí˜„ë“¤
        credible_phrases = [
            'ì—°êµ¬ì— ë”°ë¥´ë©´', 'ê³¼í•™ì ìœ¼ë¡œ', 'í†µê³„ì ìœ¼ë¡œ', 'ì‹¤í—˜ ê²°ê³¼',
            'ì „ë¬¸ê°€ê°€', 'í•™ìˆ ì ìœ¼ë¡œ', 'ê²€ì¦ëœ', 'ì…ì¦ëœ'
        ]
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‘œí˜„ë“¤
        suspicious_phrases = [
            'í™•ì‹¤íˆ', '100%', 'ì ˆëŒ€ì ìœ¼ë¡œ', 'ì˜ì‹¬ì˜ ì—¬ì§€ê°€ ì—†ë‹¤',
            'ìˆ¨ê²¨ì§„ ì§„ì‹¤', 'ìŒëª¨ë¡ ', 'ì •ë¶€ê°€ ìˆ¨ê¸´'
        ]
        
        for phrase in credible_phrases:
            if phrase in content:
                credibility += 0.05
        
        for phrase in suspicious_phrases:
            if phrase in content:
                credibility -= 0.2
        
        return max(0.0, min(1.0, credibility))
    
    def _calculate_relevance(self, result: EnhancedSearchResult, question: str) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        question_words = set(question.lower().split())
        title_words = set(result.title.lower().split())
        snippet_words = set(result.snippet.lower().split())
        
        # ì œëª©ê³¼ ì§ˆë¬¸ì˜ ë‹¨ì–´ ê²¹ì¹¨
        title_overlap = len(question_words.intersection(title_words)) / len(question_words)
        
        # ìŠ¤ë‹ˆí«ê³¼ ì§ˆë¬¸ì˜ ë‹¨ì–´ ê²¹ì¹¨
        snippet_overlap = len(question_words.intersection(snippet_words)) / len(question_words)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance = (title_overlap * 0.6 + snippet_overlap * 0.4)
        
        return min(1.0, relevance)
    
    def _calculate_fact_check_score(self, content: str) -> float:
        """ì‚¬ì‹¤ ê²€ì¦ ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì‚¬ì‹¤ ê²€ì¦ íŒ¨í„´ë“¤
        fact_patterns = [
            r'ì§€êµ¬.*êµ¬í˜•|ì§€êµ¬.*ë‘¥ê¸€', r'ë¬¼.*100ë„.*ë“', r'1\s*\+\s*1\s*=\s*2',
            r'íƒœì–‘.*ì¤‘ì‹¬', r'ì¤‘ë ¥.*ì¡´ì¬', r'DNA.*êµ¬ì¡°'
        ]
        
        total_score = 0.0
        total_weight = 0.0
        
        for pattern in fact_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                total_score += 0.8
                total_weight += 1.0
        
        if total_weight == 0:
            return 0.5  # ì¤‘ë¦½ ì ìˆ˜
        
        return total_score / total_weight
    
    def _verify_facts(self, results: List[EnhancedSearchResult], question: str) -> List[Dict]:
        """ì‚¬ì‹¤ ê²€ì¦ ìˆ˜í–‰"""
        verifications = []
        
        for result in results:
            if result.fact_check_score > 0.7:  # ë†’ì€ ì‚¬ì‹¤ ê²€ì¦ ì ìˆ˜
                verification = {
                    'statement': result.snippet,
                    'is_verified': True,
                    'confidence': result.fact_check_score,
                    'evidence': [f"íŒ¨í„´ ë§¤ì¹­: {result.title}"],
                    'verification_method': "pattern_matching",
                    'source_diversity': 1
                }
                verifications.append(verification)
        
        return verifications
    
    def _generate_comprehensive_answer(self, question: str, sources: List[EnhancedSearchResult], 
                                     fact_verifications: List[Dict]) -> Tuple[str, float, str, List[str]]:
        """ì¢…í•©ì ì¸ ë‹µë³€ ìƒì„±"""
        if not sources:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 0.0, "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ", ["ì •ë³´ ë¶€ì¡±"]
        
        # ì‹ ë¢°ë„ê°€ ë†’ì€ ì†ŒìŠ¤ë“¤ë§Œ ì‚¬ìš©
        reliable_sources = [s for s in sources if s.credibility_score > 0.7]
        
        if not reliable_sources:
            return "ì°¾ì€ ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.", 0.3, "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ë¶€ì¡±", ["ì‹ ë¢°ë„ ë‚®ìŒ"]
        
        # ë‹µë³€ êµ¬ì„±
        answer_parts = []
        confidence_scores = []
        limitations = []
        
        # ì£¼ìš” ë‹µë³€
        answer_parts.append(f"ì§ˆë¬¸: {question}")
        answer_parts.append("\në‹µë³€:")
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ë“¤ì˜ ì •ë³´ í†µí•©
        for i, source in enumerate(reliable_sources[:3], 1):
            answer_parts.append(f"{i}. {source.snippet}")
            confidence_scores.append(source.credibility_score)
        
        # ì‚¬ì‹¤ ê²€ì¦ ê²°ê³¼ ì¶”ê°€
        if fact_verifications:
            verified_count = len([v for v in fact_verifications if v['is_verified']])
            answer_parts.append(f"\nê²€ì¦ëœ ì‚¬ì‹¤: {verified_count}ê°œ í•­ëª© í™•ì¸ë¨")
            confidence_scores.append(0.9)
        
        # ì†ŒìŠ¤ ì •ë³´
        answer_parts.append(f"\nì°¸ê³  ì†ŒìŠ¤:")
        for source in reliable_sources[:3]:
            answer_parts.append(f"â€¢ {source.title} (ì‹ ë¢°ë„: {source.credibility_score:.2f})")
        
        answer = "\n".join(answer_parts)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.5
        
        # ì¶”ë¡  ê³¼ì •
        reasoning = f"ì´ {len(sources)}ê°œì˜ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìœ¼ë©°, ê·¸ ì¤‘ {len(reliable_sources)}ê°œì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
        
        # í•œê³„ì 
        if len(reliable_sources) < 3:
            limitations.append("ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ê°€ ë¶€ì¡±í•¨")
        if avg_confidence < 0.8:
            limitations.append("ì •ë³´ì˜ ì‹ ë¢°ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ")
        if not fact_verifications:
            limitations.append("ì‚¬ì‹¤ ê²€ì¦ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ")
        
        return answer, avg_confidence, reasoning, limitations

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” AI í–¥ìƒëœ ì›¹ ì—°êµ¬ì› ì‹œì‘")
    print("=" * 60)
    
    # ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
    def progress_callback(progress: SearchProgress):
        status_emoji = {
            'started': 'ğŸš€',
            'in_progress': 'â³',
            'completed': 'âœ…',
            'failed': 'âŒ'
        }
        print(f"{status_emoji.get(progress.status, 'ğŸ“')} {progress.step}: {progress.description}")
        if progress.details:
            print(f"   â””â”€ {progress.details}")
    
    researcher = AIEnhancedResearcher(progress_callback=progress_callback)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = "ì§€êµ¬ëŠ” ë‘¥ê¸€ê¹Œìš”?"
    
    print(f"\nâ“ ì§ˆë¬¸: {test_question}")
    print("-" * 40)
    
    try:
        result = researcher.research_question(test_question)
        
        print(f"\nğŸ’¡ ë‹µë³€:\n{result.answer}")
        print(f"\nğŸ¯ ì‹ ë¢°ë„: {result.confidence:.2f}")
        print(f"ğŸ” ì¶”ë¡ : {result.reasoning}")
        print(f"ğŸ“š ì†ŒìŠ¤ ìˆ˜: {len(result.sources)}ê°œ")
        print(f"âœ… ì‚¬ì‹¤ ê²€ì¦: {len(result.fact_verifications)}ê°œ")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {result.total_processing_time:.2f}ì´ˆ")
        
        if result.limitations:
            print(f"âš ï¸ í•œê³„ì : {', '.join(result.limitations)}")
        
        print(f"\nğŸ“Š ê²€ìƒ‰ ì§„í–‰ ê³¼ì •:")
        for progress in result.search_progress:
            status_emoji = {
                'started': 'ğŸš€',
                'in_progress': 'â³',
                'completed': 'âœ…',
                'failed': 'âŒ'
            }
            print(f"  {status_emoji.get(progress.status, 'ğŸ“')} {progress.step}: {progress.description}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
